{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaVZE2JubDqnWkYDMo4Yev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/logantaylor19/MLB/blob/main/DigitClassifierProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "RRpkzSqqb63X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "OX5ET2HVtBM-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loaders = {\n",
        "    'train' : DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'test'  : DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
        "}"
      ],
      "metadata": {
        "id": "thO-Hbd-up-L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.softmax(x)"
      ],
      "metadata": {
        "id": "dE4hFYGvvE0T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 20 == 0:\n",
        "          print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in loaders['test']:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target).item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(loaders['test'].dataset)\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%)\\n')"
      ],
      "metadata": {
        "id": "vTMf242Mxpd1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 11):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUjVMLEa21Wn",
        "outputId": "ba8b7264-567f-4da5-ebc0-8bd3a5321aee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7595c595093e>:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302732\n",
            "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.289521\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.204948\n",
            "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.970239\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.921793\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.842184\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.771195\n",
            "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.770280\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.758083\n",
            "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.737552\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.725668\n",
            "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.718796\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.665683\n",
            "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.675524\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.668887\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.590807\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.658629\n",
            "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.581821\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.594257\n",
            "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.651344\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.611120\n",
            "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.625705\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.619944\n",
            "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 1.597012\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.564728\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.625484\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.637570\n",
            "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.620752\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.575897\n",
            "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1.609920\n",
            "Test set: Average loss: 0.0152, Accuracy: 9412/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.569995\n",
            "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 1.572119\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1.579844\n",
            "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1.555305\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1.572391\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.581078\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.608681\n",
            "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 1.602739\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.629109\n",
            "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 1.577899\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.547998\n",
            "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 1.607452\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.573935\n",
            "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 1.582236\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 1.551098\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.567917\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.549289\n",
            "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 1.595826\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 1.539146\n",
            "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 1.632668\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.595360\n",
            "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 1.575177\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 1.561150\n",
            "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 1.617988\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.579492\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.509215\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 1.583566\n",
            "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 1.616002\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 1.574916\n",
            "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 1.531550\n",
            "Test set: Average loss: 0.0151, Accuracy: 9536/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.507307\n",
            "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 1.611857\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 1.610276\n",
            "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 1.560438\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 1.558151\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.560378\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 1.569459\n",
            "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 1.580440\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.559149\n",
            "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 1.620588\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.557226\n",
            "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 1.535524\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.575409\n",
            "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 1.616872\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 1.561688\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.598562\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.593211\n",
            "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 1.559917\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 1.552721\n",
            "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 1.572950\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.525127\n",
            "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 1.510314\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 1.530419\n",
            "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 1.566675\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.537386\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.545824\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 1.608330\n",
            "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 1.557757\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 1.558854\n",
            "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 1.510601\n",
            "Test set: Average loss: 0.0150, Accuracy: 9626/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.579206\n",
            "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 1.548535\n",
            "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 1.539192\n",
            "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 1.547014\n",
            "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 1.528407\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.569152\n",
            "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.578384\n",
            "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 1.545611\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.552240\n",
            "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 1.519353\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.540262\n",
            "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 1.575700\n",
            "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 1.574926\n",
            "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 1.550617\n",
            "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 1.543291\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.592372\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.573568\n",
            "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 1.561827\n",
            "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.535530\n",
            "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 1.514667\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.502207\n",
            "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 1.543523\n",
            "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.585255\n",
            "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 1.517975\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.555589\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.596964\n",
            "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 1.554372\n",
            "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 1.489670\n",
            "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 1.526978\n",
            "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 1.573316\n",
            "Test set: Average loss: 0.0150, Accuracy: 9657/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.533407\n",
            "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 1.556806\n",
            "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 1.538199\n",
            "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 1.524415\n",
            "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 1.529584\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.525882\n",
            "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 1.538201\n",
            "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 1.561334\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.557813\n",
            "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 1.555929\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.540664\n",
            "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 1.542338\n",
            "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 1.522081\n",
            "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 1.532432\n",
            "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 1.558209\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.521224\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.526043\n",
            "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 1.592937\n",
            "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.539036\n",
            "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 1.496595\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.569134\n",
            "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 1.539045\n",
            "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 1.557945\n",
            "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 1.578124\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.494822\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.552676\n",
            "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 1.569920\n",
            "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.541947\n",
            "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 1.520257\n",
            "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 1.523177\n",
            "Test set: Average loss: 0.0150, Accuracy: 9660/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.513312\n",
            "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 1.556903\n",
            "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 1.526456\n",
            "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 1.539226\n",
            "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 1.535058\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.550292\n",
            "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 1.545712\n",
            "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 1.517309\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.513251\n",
            "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 1.549713\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.575595\n",
            "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 1.584432\n",
            "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 1.509387\n",
            "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 1.546165\n",
            "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 1.546161\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.591450\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.528217\n",
            "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 1.527973\n",
            "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 1.514483\n",
            "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 1.507250\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.534698\n",
            "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 1.498526\n",
            "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 1.572544\n",
            "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 1.581304\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 1.519058\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.527039\n",
            "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 1.528170\n",
            "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 1.545469\n",
            "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 1.562135\n",
            "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 1.501961\n",
            "Test set: Average loss: 0.0149, Accuracy: 9692/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.555814\n",
            "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 1.538371\n",
            "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 1.574599\n",
            "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 1.517771\n",
            "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 1.520667\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.530922\n",
            "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 1.515648\n",
            "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 1.569027\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.529258\n",
            "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 1.540300\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.586320\n",
            "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 1.545392\n",
            "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 1.527928\n",
            "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 1.500170\n",
            "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 1.514301\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.500623\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.511650\n",
            "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 1.551042\n",
            "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 1.507489\n",
            "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 1.530173\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.537243\n",
            "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 1.568638\n",
            "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 1.557966\n",
            "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 1.533356\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.554852\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.543544\n",
            "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 1.574584\n",
            "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 1.548544\n",
            "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 1.555533\n",
            "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 1.522474\n",
            "Test set: Average loss: 0.0149, Accuracy: 9713/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.526254\n",
            "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 1.541213\n",
            "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 1.496389\n",
            "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 1.509743\n",
            "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 1.548977\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.544731\n",
            "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 1.500585\n",
            "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 1.466689\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 1.552320\n",
            "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 1.536080\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.515759\n",
            "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 1.531486\n",
            "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 1.529927\n",
            "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 1.571782\n",
            "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 1.551302\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.526317\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.562366\n",
            "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 1.532043\n",
            "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 1.523851\n",
            "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 1.523868\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.528412\n",
            "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 1.481302\n",
            "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 1.532781\n",
            "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 1.540059\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 1.501741\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.588902\n",
            "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 1.543205\n",
            "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 1.510458\n",
            "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 1.529395\n",
            "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 1.542637\n",
            "Test set: Average loss: 0.0149, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.560691\n",
            "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 1.560825\n",
            "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 1.520889\n",
            "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 1.515706\n",
            "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 1.538089\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.536914\n",
            "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 1.508358\n",
            "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 1.536599\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 1.518058\n",
            "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 1.499294\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.545145\n",
            "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 1.511469\n",
            "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 1.516429\n",
            "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 1.550790\n",
            "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 1.522742\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.501473\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.512190\n",
            "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 1.496250\n",
            "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 1.532322\n",
            "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 1.556658\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.526713\n",
            "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 1.527517\n",
            "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 1.548283\n",
            "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 1.554060\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.517530\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.550317\n",
            "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 1.536576\n",
            "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 1.527240\n",
            "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 1.524604\n",
            "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 1.516931\n",
            "Test set: Average loss: 0.0149, Accuracy: 9732/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.487328\n",
            "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 1.512361\n",
            "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 1.499078\n",
            "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 1.553265\n",
            "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 1.514120\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.513830\n",
            "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 1.519317\n",
            "Train Epoch: 10 [14000/60000 (23%)]\tLoss: 1.496258\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.517236\n",
            "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 1.555034\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.524659\n",
            "Train Epoch: 10 [22000/60000 (37%)]\tLoss: 1.535906\n",
            "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 1.539894\n",
            "Train Epoch: 10 [26000/60000 (43%)]\tLoss: 1.519745\n",
            "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 1.499074\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.497819\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.518833\n",
            "Train Epoch: 10 [34000/60000 (57%)]\tLoss: 1.524653\n",
            "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 1.524519\n",
            "Train Epoch: 10 [38000/60000 (63%)]\tLoss: 1.525701\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.530759\n",
            "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 1.497226\n",
            "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 1.543535\n",
            "Train Epoch: 10 [46000/60000 (77%)]\tLoss: 1.511173\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.482917\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.544557\n",
            "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 1.504794\n",
            "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 1.507554\n",
            "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 1.515224\n",
            "Train Epoch: 10 [58000/60000 (97%)]\tLoss: 1.513754\n",
            "Test set: Average loss: 0.0149, Accuracy: 9746/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "data, target = test_data[8]   #change this number to show prediction for different images in test data\n",
        "\n",
        "data = data.unsqueeze(0).to(device)\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "prediction = output.argmax(dim=1, keepdim=True).item()\n",
        "\n",
        "print(f'Prediction: {prediction}')\n",
        "\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "X-6mFypk4Vlv",
        "outputId": "6dd09aee-fb0a-442a-e120-7e42e077aeee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7595c595093e>:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}